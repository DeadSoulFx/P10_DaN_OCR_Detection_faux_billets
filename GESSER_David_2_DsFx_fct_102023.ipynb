{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8600566a-b498-4aa8-bb13-ff243a6a22b0",
   "metadata": {},
   "source": [
    "# ***Fonctions utiles***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8171d0e0-0361-4f5e-895b-53e9b374baec",
   "metadata": {},
   "source": [
    "---\n",
    "## Partie 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6781bb31-3048-4370-8620-32b25a18fea8",
   "metadata": {},
   "source": [
    "### 1. Import des librairies (ok)\n",
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "id": "02392769-0903-418d-9a05-0459bdc23d40",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Mes imports de lib de base\n",
    "import pandas as pd # Pour travail avec les df\n",
    "import numpy as np # Tout ce qui est fct math avec numpy, tjs utile\n",
    "\n",
    "# Pour les graphiques\n",
    "import matplotlib.pyplot as plt # Graphes avec matplotlib\n",
    "import seaborn as sns # Graphes avec seaborn\n",
    "sns.set_style('darkgrid', {'grid.color': '.5', 'grid.linestyle': ':'}) # Défini un fond de graphe que ce soit pour sns ou plt\n",
    "\n",
    "# Pour les stats et autre\n",
    "import scipy.stats as stats # Pour des stats avec scipy\n",
    "import statsmodels.api as sm # Pour des régressions linéaires ou autre\n",
    "\n",
    "# Pour le clustering et PCA\n",
    "import sklearn as sk\n",
    "import scipy as sp\n",
    "# import pca as pca\n",
    "from sklearn.cluster import KMeans # Clustering avec KMeans\n",
    "from sklearn.decomposition import PCA # Pour calcul des composantes principales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff0f370e-ade1-4e6e-bfb4-bbdfe0533c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selon le projet ces librairies pourront êtres ajustées en fonction des besoins (ajout, suppression) - J'essaie de couvrir la majorité des besoins\n",
    "# Nppt :\n",
    "    # Faire un tri dans toutes ces lib et les ranger correctement pour un meilleur visu\n",
    "    # Mettre les libs non utiles en commentaire pour les garder sous la main au cas ou\n",
    "# Librairies de bases\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import scipy as sp\n",
    "import matplotlib\n",
    "import statsmodels\n",
    "\n",
    "# Scipy pour stats\n",
    "from scipy.stats import pearsonr # Coeff de pearson\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "# StatModel\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# SKlearn pour PCA et clustering et autre\n",
    "from sklearn.decomposition import PCA # Pour faire une ACP\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler # Scalling des données\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score, confusion_matrix\n",
    "from sklearn.cluster import KMeans # Clustering KMeans\n",
    "from sklearn import preprocessing, decomposition\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.cluster import AgglomerativeClustering # Clustering CAH\n",
    "from sklearn.metrics import f1_score # Pour le calcul du F1-Score des modèles\n",
    "\n",
    "# Librairie pour les graphs et autres\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid', {'grid.color': '.5', 'grid.linestyle': ':'}) # Défini un fond de graphe que ce soit pour sns ou plt\n",
    "\n",
    "# Librairies diverses\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor # Calcul de la corrélation entre nos variables choisies (Variance Inflation Factor)\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.model_selection import  GridSearchCV # Recherche de param pour modèles de ML\n",
    "\n",
    "    # Librairie nous permettant de faire une Régression Linéaire et Logistique\n",
    "from sklearn.model_selection import train_test_split # Split des données pour train des modeles\n",
    "from sklearn.linear_model import LogisticRegression # Regression logistique SKLearn\n",
    "from sklearn.linear_model import LinearRegression # Regression linéaire SKLearn\n",
    "    # Metrics pour évaluation de modèles ML\n",
    "from sklearn.metrics import roc_auc_score , roc_curve, accuracy_score , precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "\n",
    "\n",
    "    # Librairies nécessaire pour réaliser une matrice de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Pour stocker des fonctions à part et les réutiliser\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7081bda1-810e-4cda-b3bc-a84d8fe61e26",
   "metadata": {},
   "source": [
    "#### *Vérification import lib et run des fct*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "862432e4-cc8f-459e-86c5-0433f700b212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test pour voir si le book des fonctions est bien lancé et les libs bien chargées\n",
    "# Affiche la version des librairies chargées\n",
    "def fct_test():\n",
    "    \"\"\"\n",
    "    Vérification de l'importation des librairies et du run du book de fonctions\n",
    "    Affichage des versions des librairies chargées\n",
    "    \n",
    "    \n",
    "    return :\n",
    "    -----------------------------------------------------------------------------------------------\n",
    "        Version des librairies chargées\n",
    "        Message de confirmation du chargement des librairies et des fonctions\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Librairies utilisées :')\n",
    "    print('+-------------------------------------+')\n",
    "    print('    Pandas :', pd.__version__)\n",
    "    print('     Numpy :', np.__version__)\n",
    "    print('Matplotlib :', matplotlib.__version__)\n",
    "    print('   Seaborn :', sns.__version__)\n",
    "    print('     Spicy :', sp.__version__)\n",
    "    print('Statmodels :', statsmodels.__version__)\n",
    "    print('   Sklearn :', sk.__version__)\n",
    "    print('    JobLib :', joblib.__version__)\n",
    "    print('+-------------------------------------+')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d6cd56-5e7a-4f10-8d7b-b3e39c6b613b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f6be05-0782-43a9-a41a-f0b90c93d82a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9593b3d-f989-41b3-966b-fbe7b5605724",
   "metadata": {},
   "source": [
    "#### *Divers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "341065e7-d3c5-4b74-be1e-7ca83184d3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Longeur max pour infos sur fonction : ----------------------------------------------------------------------------------------------- (95 caractères depuis première marge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518c19ab-01af-4bf4-a879-ecf2cf4e2d39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb39e532-3888-4b4d-9c8c-0db93dda02dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d2d32b-6bd7-476d-ba8a-d5ee2416020a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f67848-0a44-4136-b1c0-ba673e9e1d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80507be6-1d87-4239-8534-8f9fed8a8570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d3c8456-e16e-41b0-858e-356459283196",
   "metadata": {},
   "source": [
    "---\n",
    "## Partie 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b828a42-b979-4053-b689-acb9e42d09de",
   "metadata": {},
   "source": [
    "### 1. Fonctions utiles\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4475c251-4583-4c02-b239-a21bf4f73f00",
   "metadata": {},
   "source": [
    "#### *Infos de bases*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d50d578b-d8a2-4c16-8721-8e45235456f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Première infos sur un dataset, info(), isna(), describe()\n",
    "def fct_first_look(df):\n",
    "    \"\"\"\n",
    "    Première visualisation du df\n",
    "    Renvoi des informations de bases comme .info(), .describe() ou encore le nombre de manquants par variables\n",
    "    \n",
    "    Positional arguments : \n",
    "    -----------------------------------------------------------------------------------------------\n",
    "    arg :\n",
    "        df (pandas.DataFrame) - DataFrame contenant les données à analyser \n",
    "    \n",
    "    return :\n",
    "    -----------------------------------------------------------------------------------------------\n",
    "        display .info(), .isna().sum() et .describe(include = 'all')\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Visu du dataset (entête) :')\n",
    "    display(df.sample(4))\n",
    "    print('')\n",
    "    print('+-----------------------------------+')\n",
    "    print('Infos sur le df :')\n",
    "    display(df.info())\n",
    "    print('')\n",
    "    print('+-----------------------------------+')\n",
    "    print('Nombre de valeurs NaN')\n",
    "    display(df.isna().sum())\n",
    "    print('')\n",
    "    print('+-----------------------------------+')\n",
    "    print(\"Déscription du df (include = 'all')\")\n",
    "    display(round(df.describe(include = 'all'), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16de718-3ae3-4df1-8346-1288821a7dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e1fb1ce-1abe-4db5-93de-a5228868a938",
   "metadata": {},
   "source": [
    "#### *Triangle de corrélations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff1a3f49-6883-4689-b276-2d64c1a71969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identique qu'une Heatmap mais visualisation différente\n",
    "def fct_corr_matrix(df, annot = True):\n",
    "    \"\"\"\n",
    "    Affiche la matrice de corrélation entre les données, reprend la heatmap mais la découpe\n",
    "    en forme de triangle\n",
    "    \n",
    "    Positional arguments : \n",
    "    -----------------------------------------------------------------------------------------------\n",
    "    arg :\n",
    "        df (pandas.DataFrame) - DataFrame contenant les données à analyser \n",
    "    \n",
    "    return :\n",
    "    -----------------------------------------------------------------------------------------------\n",
    "        Triangle de corrélation entre les variables\n",
    "    \"\"\"\n",
    "    \n",
    "    # Matrice de corrélation\n",
    "    matrix = df.corr()\n",
    "\n",
    "    # Triangle de corrélations\n",
    "    mask = np.triu(np.ones_like(matrix, dtype = bool))\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.title('Triangle des corrélations', fontdict = {'fontsize' : '14', 'color' : 'black', 'fontweight' : 'bold'})\n",
    "    sns.heatmap(matrix, mask = mask, annot = annot, cmap = 'RdGy')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5dda84-2269-4848-a332-63500d6ec65c",
   "metadata": {},
   "source": [
    "#### *Heatmap*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5039e5ab-6abb-4000-979c-e130df130f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identique que triangle de corrélation mais visualisation complète\n",
    "def fct_corr_heatmap(data, annot=True):\n",
    "    \"\"\"\n",
    "    Calcule le coefficient de corrélation de Pearson entre toutes les paires de variables d'un df\n",
    "    \n",
    "    Positional arguments : \n",
    "    -----------------------------------------------------------------------------------------------\n",
    "    arg :\n",
    "        df (pandas.DataFrame) - DataFrame contenant les données à analyser\n",
    "    \n",
    "    return :\n",
    "    -----------------------------------------------------------------------------------------------\n",
    "        Heatmap contenant les coefficients de corrélation entre toutes les paires de variables\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(12,6))\n",
    "    \n",
    "    plt.title(\"Heatmap de corrélation de Pearson\", fontdict = {'fontsize' : '14', 'color' : 'black', 'fontweight' : 'bold'})\n",
    "    sns.heatmap(data.corr(), annot = annot, cmap = 'RdGy')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f87303d-e45d-45df-a96e-9406bc467fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafb00f9-9ddf-4667-a9a1-40f76ee3c56c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c90ce2cc-ed4e-4bc7-8b0e-6cdaac63d70f",
   "metadata": {},
   "source": [
    "#### *Scaling des données*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "022d3508-6ad9-448d-9e37-349e45285fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale les variables quantitative selon méthode sélectionnée\n",
    "def fct_data_scaler(df, columns = None, method = 'standard', scaler_all = False):\n",
    "    \"\"\"\n",
    "    Scale les données d'un DataFrame en utilisant différentes méthodes de scaling,\n",
    "    possibilité de sélection des colonnes\n",
    "    \n",
    "    Positional arguments : \n",
    "    -----------------------------------------------------------------------------------------------\n",
    "    arg :\n",
    "        df (pd.DataFrame) : DataFrame contenant les données à scaler\n",
    "        columns (list): Liste des noms de colonnes à scaler. Si aucune liste n'est spécifiée,\n",
    "        toutes les colonnes sont scalées (par défaut: None)\n",
    "        \n",
    "        method (str): Méthode de scaling à utiliser. Les valeurs possibles sont 'standard' (par défaut),\n",
    "        'minmax', 'robust' et 'log'\n",
    "        \n",
    "        scaler_all (bool): Si True, scaler toutes les colonnes du DataFrame (par défaut: False)\n",
    "    \n",
    "    return :\n",
    "    -----------------------------------------------------------------------------------------------\n",
    "        pd.DataFrame : DataFrame contenant les données scalées\n",
    "    \"\"\"\n",
    "    \n",
    "    # On vérifie si on doit appliquer le scaler sur toutes nos données\n",
    "    if columns is None and not scaler_all:\n",
    "        raise ValueError(\"Spécifiez les colonnes à scaler ou activez l'option 'scaler_all' pour scaler toutes les colonnes.\")\n",
    "    \n",
    "    # Si scaller_all == True, on travail sur toutes les colonnes\n",
    "    if scaler_all:\n",
    "        columns = df.columns.tolist()\n",
    "    \n",
    "    # On sélectionne la méthode de Scalage et on l'applique à nos données.\n",
    "    if method == \"standard\":\n",
    "        scaler = StandardScaler()\n",
    "    elif method == \"minmax\":\n",
    "        scaler = MinMaxScaler()\n",
    "    elif method == \"robust\":\n",
    "        scaler = RobustScaler()\n",
    "    elif method == \"log\":\n",
    "        def log_scaler(data):\n",
    "            return np.log1p(data)\n",
    "        scaler = log_scaler\n",
    "    else:\n",
    "        raise ValueError(\"Méthode de scaling non valide. Les valeurs possibles sont 'standard', 'minmax', 'robust' et 'log'.\")\n",
    "\n",
    "    # On scale nos données avec la méthode sélectionnée    \n",
    "    if method == \"log\":\n",
    "        df_scaled = df[columns].apply(scaler)\n",
    "    else:\n",
    "        scaler.fit(df[columns])\n",
    "        scaled_data = scaler.transform(df[columns])\n",
    "        df_scaled = pd.DataFrame(scaled_data, columns = columns, index = df.index)\n",
    "    \n",
    "    # On remplace nos données initiales par les données scalées tout en conservant les données que nous ne voulions pas traiter\n",
    "    df = pd.concat([df.drop(columns, axis=1), df_scaled], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e3ab25-1966-4c50-be1a-dde20d8dae28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fff467-3196-44ff-8c95-535cc9fc7de6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db602f36-7d16-4744-a67d-3558a09333af",
   "metadata": {},
   "source": [
    "#### *IQR sur une variable*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9855f3d-8ed0-4a49-ad82-924d042699ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de recherche des outliers avec la méthode des IQR\n",
    "# En cours de dévelloppement\n",
    "# Source : From Scratch\n",
    "def fct_outliers_iqr(data,  disp = True):\n",
    "    \"\"\"\n",
    "    data (pd.series or np.ndarray) :\n",
    "        Colonne d'un df ou nd.array contenant les données à traiter\n",
    "    \n",
    "    Positional arguments : \n",
    "    -----------------------------------------------------------------------------------------------\n",
    "    data (pd.series or np.array) :\n",
    "        Variable à tester\n",
    "    disp (bool) :\n",
    "        Affiche ou non le df avec les outliers (défaut = True)\n",
    "        \n",
    "    return :\n",
    "    -----------------------------------------------------------------------------------------------\n",
    "        outlier ((pd.DataFrame) :\n",
    "            Dataframe contenant la liste des outliers et leurs index\n",
    "        \n",
    "        La liste des index des outliers\n",
    "        Un dataframe listant tous les outliers dépassant 1.5x IQR (si disp = True)\n",
    "        lower_bound, upper_bound\n",
    "    \"\"\"\n",
    "    \n",
    "    # Faire un test des données d'entrées (test sur le type)\n",
    "    \n",
    "    # On calcule l'interquartile (on pourrait passer un arg pour définir selon besoin)\n",
    "    q1, q3 = np.percentile(data, [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    print('Quartilles :')\n",
    "    print('')\n",
    "    print(\"Q1 => \" + str(round(q1,  4)))\n",
    "    print(\"Q3 => \" + str(round(q3, 4)))\n",
    "    print(\"IQR => \" + str(round(iqr, 4)))\n",
    "    \n",
    "    # Calcul des valeurs limites (on pourrait passer un arg pour les définir selon besoin)\n",
    "    lower_bound = q1 -(1.5 * iqr)\n",
    "    upper_bound = q3 +(1.5 * iqr)\n",
    "    print(\"Lower_bound => \" + str(round(lower_bound, 4)))\n",
    "    print(\"Upper_bound => \" + str(round(upper_bound, 4)))\n",
    "\n",
    "    # Outliers\n",
    "    print('')\n",
    "    print('--------------------------------------------------------------')\n",
    "    print('Liste des outliers selon la méthode des écarts interquartilles')\n",
    "    print('--------------------------------------------------------------')\n",
    "    print('')\n",
    "    outlier = data[(data > upper_bound) | (data < lower_bound)]\n",
    "    # On passe le résultat sous forme de dataframe\n",
    "    outlier = pd.DataFrame(outlier)\n",
    "    \n",
    "    # A améliorer pour ne pas avoir de double affichage parfois\n",
    "    if disp == True:\n",
    "        display(outlier)\n",
    "    \n",
    "    return outlier.index.tolist(), lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e13cdbfb-a1a1-454a-b78c-117118b10c3a",
   "metadata": {},
   "source": [
    "# On fait un boxplot sur les différentes colonnes de notre df\n",
    "df_final.columns.tolist()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8c4f0d24-f5a4-4d9f-a2f0-9c83e47e1a20",
   "metadata": {},
   "source": [
    "# Au vu du boxplot, il serait peut être plus intéressant de chercher des outliers sur un df standardisé\n",
    "# Sur ce boxplot les echelles de données sont différentes donc la visualisation globale est impossible\n",
    "plt.figure(figsize = (21,6))\n",
    "sns.boxplot(data = df_final, orient = 'h', palette = 'OrRd')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "17e246ef-06f9-492e-b797-69ac7d8289d2",
   "metadata": {},
   "source": [
    "df_final.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc1f47a-e71a-427f-aefc-863d7ec30603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5484ef-51cc-4df3-a497-ee4dca711aec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9b6e4f-b730-469b-8213-41c83e1b67e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f13f93ed-9147-4db2-8a70-d5e12b1b60e5",
   "metadata": {},
   "source": [
    "#### *Variance Inflation Factor*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be467895-46b8-413e-9b37-5b0089ca1c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source : From Scratch\n",
    "def fct_vif(data):\n",
    "    \"\"\"\n",
    "    Effectue un VIF sur les données du dataframe\n",
    "\n",
    "    Parameters : \n",
    "    -----------------------------------------------------------------------------------------------\n",
    "    data :\n",
    "        pandas.DataFrame : DataFrame contenant les données quantitatives\n",
    "    \n",
    "    return :\n",
    "    -----------------------------------------------------------------------------------------------\n",
    "        pandas.DataFrame : DataFrame contenant les données VIF\n",
    "        Affichage uniquement\n",
    "    \n",
    "    Infos :\n",
    "    -----------------------------------------------------------------------------------------------\n",
    "    /!\\ Le dataframe d'entrée ne doit pas contenir de valeurs NaN, de str ou de bool\n",
    "    \n",
    "    Dépendance linéaire entre les variables :\n",
    "    \n",
    "    If the degree of correlation is high enough between variables, it can cause problems when\n",
    "    fitting and interpreting the regression model\n",
    "    \n",
    "    A general rule of thumb for interpreting VIFs is as follows :\n",
    "    \n",
    "        A value of 1 indicates there is no correlation between a given explanatory variable and\n",
    "        any other explanatory variables in the model\n",
    "    \n",
    "        A value between 1 and 5 indicates moderate correlation between a given explanatory variable\n",
    "        and other explanatory variables in the model,but this is often not severe enough to\n",
    "        require attention\n",
    "        \n",
    "        A value greater than 5 indicates potentially severe correlation between a given explanatory\n",
    "        variable and other explanatory variables in the model\n",
    "        \n",
    "        One way to detect multicollinearity is by using a metric known as the\n",
    "        variance inflation factor (VIF), which measures the correlation and strength of\n",
    "        correlation between the explanatory variables in a regression model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Test sur le type de variables à traiter\n",
    "    for i in data.columns:\n",
    "        # Test sur type de variable\n",
    "        if data[i].dtypes != 'float64' and data[i].dtypes != 'int64':\n",
    "            print('Type non supporté :', data[i].dtypes)\n",
    "            print('-------------------------------------')\n",
    "            print(data.dtypes)\n",
    "            print('Erreur, veuillez vérifier les données')\n",
    "            raise ValueError('Le df contient des données non numériques...')\n",
    "            break\n",
    "        \n",
    "        # Test des NaN\n",
    "        elif data[i].isna().sum() != 0:\n",
    "            print('Nombre de NaN :', data[i].isna().sum())\n",
    "            print('-------------------------------------')\n",
    "            print(data.isna().sum())\n",
    "            print('Erreur, veuillez vérifier les données')\n",
    "            raise ValueError('Le df contient des données manquantes...')\n",
    "            break\n",
    "        \n",
    "        # Validation de la colonne   \n",
    "        else:\n",
    "            print('')\n",
    "            print('Variable vérifiée :', i)\n",
    "            print('Ok pour traitement des données après vérifcation...')\n",
    "    \n",
    "    # VIF dataframe\n",
    "    vif_data = pd.DataFrame() # On créé un df vide\n",
    "    vif_data['feature'] = data.columns # On prend les colonnes de notre df\n",
    "      \n",
    "    # Calcul du VIF pour chaque feature\n",
    "    vif_data['VIF'] = [variance_inflation_factor(data.values, i) for i in range(len(data.columns))]\n",
    "    \n",
    "    # df de sortie avec les valeurs VIF\n",
    "    print('')\n",
    "    print('+---------------------------------+')\n",
    "    print('| VIF - Variance Inflation Factor |')\n",
    "    print('+---------------------------------+')\n",
    "    display(vif_data)\n",
    "    # Faire une rapide explication du VIF\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a938fa40-96b0-4a4c-8a27-7319c75e8ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c53d19-dfd9-447b-8241-4b340016e2b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69a3f29-60e4-4f8f-85e3-0f20c2e7aad3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3416b843-9eca-4f37-ad92-a96b96726076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b8c4c0-509e-4a1a-a567-b5b777b8b4ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31691db4-c23a-44b0-9330-84836b7d1f92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1766b9af-94f6-4fd0-a8ed-ea90343ed4b0",
   "metadata": {},
   "source": [
    "#### *Shapiro - Test de normalité*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6010368d-7a94-44e6-a528-c61555601fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source : From Scratch\n",
    "def fct_shapiro(data, graph = True, bins = 20, color = 'Red'):\n",
    "    \"\"\"\n",
    "    Effectue un test de shapiro afin de vérifier si la distribution des variables suit une\n",
    "    loi normale\n",
    "    Test possible sur plusieurs colonnes d'un pd.DataFrame mais ne renverra que le graphique\n",
    "\n",
    "    Parameters : \n",
    "    -----------------------------------------------------------------------------------------------\n",
    "    data (pandas.DataFrame or np.ndarray) :\n",
    "        Array of sample data, possible de passer un df complet ou certaines colonnes avec [[,]]\n",
    "    graph (bool) :\n",
    "        Choix d'afficher le graphique ou non, par défaut = True\n",
    "    bins (int) :\n",
    "        Spécifie le nombre de bins pour l'histogramme, par défaut = 20\n",
    "    color (str) :\n",
    "        Permet de choisir la couleur du graphique\n",
    "    \n",
    "    return :\n",
    "    -----------------------------------------------------------------------------------------------\n",
    "        tuple containing :\n",
    "            statistic : float\n",
    "                The test statistic\n",
    "            p-value : float\n",
    "                The p-value for the hypothesis test\n",
    "        \n",
    "        sns.histplot : Graphique de la distribution de la variable\n",
    "    \n",
    "    Infos :\n",
    "    -----------------------------------------------------------------------------------------------\n",
    "    Test Shapiro, test pour vérifier la normalité d'UNE variable\n",
    "\n",
    "        H0 : la distribution suit une loi normale\n",
    "        H1 : la distribution ne suit pas une loi normale\n",
    "\n",
    "        Si p-value > 0.05, hypothèse H0 acceptée, distribution normale\n",
    "        Si p-value < 0.05, hypothèse H0 rejectée, distribution pas normale\n",
    "        \n",
    "        Si plusieurs variables sont passées en entrée, n'éffectue pas le test mais retourne un\n",
    "        histogramme avec les différences variables\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Génération du graphique de la distribution sous forme d'histogramme\n",
    "    if graph == True:\n",
    "        plt.figure(figsize=(16,6))\n",
    "        \n",
    "        sns.histplot(data = data, kde = True, color = color, bins = bins)\n",
    "        plt.title('Histogramme de distribution', fontdict = {'fontsize' : '14', 'color' : 'black', 'fontweight' : 'bold'})\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    # Si ce n'est pas une série n'affichera pas les infos du test Shapiro\n",
    "    # Print des résultats si Shapiro sur une seule variable\n",
    "    if type(data) ==  pd.core.series.Series or type(data) == np.ndarray:\n",
    "        # On effectue le test et on récupère les valeurs retournées\n",
    "        # Récupération de stats\n",
    "        stat_shapiro = stats.shapiro(data)[0]\n",
    "        # Récupération de la p-value\n",
    "        pvalue_shapiro  = stats.shapiro(data)[1]\n",
    "        \n",
    "        print('Résultats du test Shapiro :')\n",
    "        print('-----------------------------------------------------------------------------------------------')\n",
    "        print('Statistique :', round(stat_shapiro, 3))\n",
    "        print('p-value :', pvalue_shapiro)\n",
    "        print(\"Nombre d'individus :\", len(data), \"(Attention, si nombre d'individus élevés)\")\n",
    "        print('-----------------------------------------------------------------------------------------------')\n",
    "    \n",
    "        # Analyse des résultats\n",
    "        print('')\n",
    "        print('Note :')\n",
    "        print('-----------------------------------------------------------------------------------------------')\n",
    "        print('H0 : la distribution suit une loi normale')\n",
    "        print('H1 : la distribution ne suit pas une loi normale')\n",
    "        print('')\n",
    "        print('Si p-value > 0.05, hypothèse H0 acceptée, distribution normale')\n",
    "        print(\"Si p-value < 0.05, hypothèse H0 rejetée, distribution n'est pas normale\")\n",
    "        print('')\n",
    "\n",
    "        if pvalue_shapiro < 0.05:\n",
    "            print('p-value < 0.05 : H0  Rejetée, la distribution ne suit pas une loi normale')\n",
    "        else:\n",
    "            print('p-value > 0.05 : H0 acceptée, la distribution suit une loi normale')\n",
    "        \n",
    "    elif  type(data) !=  pd.core.series.Series or type(data) != np.ndarray:\n",
    "        # Récupération de stats\n",
    "        stat_shapiro = 0\n",
    "        # Récupération de la p-value\n",
    "        pvalue_shapiro  = 0\n",
    "        \n",
    "        print('Trop de variables')\n",
    "    \n",
    "    # Ajouter une info sur le nombre d'individus testé pour l'appréciation du test\n",
    "    \n",
    "    # return des résultats sous forme de tuple si besoin\n",
    "    return stat_shapiro, pvalue_shapiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc04fa5-0565-45b9-b7cf-da07e6e77a83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79fbbbf2-70ac-4847-a628-a305ed7e225e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de Shapiro sur toutes les colonnes d'un dataframe\n",
    "# En cours de création, l'idée est de faire un shapiro sur chaque variables passée en entrée\n",
    "\n",
    "def fct_shapiro_multi(data, graph = True, bins = 20, hue = None, color = 'DarkRed'):\n",
    "    \"\"\"\n",
    "    Affiche la distribution de chaque variable du dataframe passé en argument\n",
    "    Pour analyser une variable particulière utiliser la fonction :\n",
    "        fct_shapiro(data, graph = True, summary = True)\n",
    "\n",
    "    Parameters : \n",
    "    -----------------------------------------------------------------------------------------------\n",
    "    data (pandas.DataFrame) :\n",
    "        Array of sample data, possible de passer un df complet ou certaines colonnes avec [[,]]\n",
    "    graph (bool) :\n",
    "        Choix d'afficher le graphique ou non, par défaut = True    \n",
    "    bins (int) :\n",
    "        Spécifie le nombre de bins pour l'histogramme, par défaut = 20\n",
    "    color (str) :\n",
    "        Permet de choisir la couleur du graphique\n",
    "    \n",
    "    return :\n",
    "    -----------------------------------------------------------------------------------------------        \n",
    "        sns.histplot : Graphique de la distribution de la variable\n",
    "    \"\"\"\n",
    "    \n",
    "    #  En premier tester les données pour savoir si possible de générer le graph\n",
    "    \n",
    "    # Histogramme de la distribution des valeurs par variables (si graph = True)\n",
    "    if graph == True:\n",
    "        for i in data.columns:\n",
    "            fig, ax = plt.subplots(1, 1, figsize = (12, 2))\n",
    "            \n",
    "            sns.histplot(ax = ax, data = data, x = i, kde = True, fill = True, hue = hue, bins = bins, color = color)\n",
    "            \n",
    "            plt.show()\n",
    "            \n",
    "            # On effectue le test et on récupère les valeurs retournées\n",
    "            # Récupération de stats\n",
    "            stat_shapiro = stats.shapiro(data)[0]\n",
    "            # Récupération de la p-value\n",
    "            pvalue_shapiro  = stats.shapiro(data)[1]\n",
    "            \n",
    "            print('Résultats du test Shapiro :', i)\n",
    "            print('-----------------------------------------------------------------------------------------------')\n",
    "            print('Statistique :', stat_shapiro)\n",
    "            print('p-value :', pvalue_shapiro)\n",
    "            print(\"Nombre d'individus :\", len(data), \"(Attention, si nombre d'individus élevés)\")\n",
    "            print('-----------------------------------------------------------------------------------------------')\n",
    "            \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54e6fb8-c101-4baa-bee3-2346bcb69b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424112d4-ca8c-4f29-aa85-e585d7c12f77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42625d0f-4846-4184-99d9-9c9e8b45b767",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e76cb1-4deb-4548-b47a-1cd4f3c5268f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1027ac-8252-4a7a-a8cf-91f89eb3ca29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c68a70-b4a5-4786-9977-9aa172c3f800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b92ebc8a-b5c4-4c69-abf7-1425fd3f9fdf",
   "metadata": {},
   "source": [
    "#### *ACP - Algo de calcul*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fbe8c4c-1428-451d-b5e2-fdeff7d4ef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul les composantes de l'ACP et le scree_cum, pour le cercle de corrélation et la projection des individus, utiliser les fonction à la suite après avoir fait l'ACP\n",
    "# Penser à scaler les données avant et à faire attention aux NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "468eee87-0267-4b54-8f58-a83d4451e7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source : From Scratch\n",
    "def fct_pca(data, n_comp = 2):\n",
    "    \"\"\"\n",
    "    Effectue une ACP sur les données\n",
    "\n",
    "    Positional arguments : \n",
    "    -----------------------------------------------------------------------------------------------\n",
    "    data :\n",
    "        pandas.DataFrame : DataFrame contenant les données scalées\n",
    "    n_comp :\n",
    "        int : nombre de composantes de l'ACP (par défaut : 2)\n",
    "    \n",
    "    return :\n",
    "    -----------------------------------------------------------------------------------------------\n",
    "        ACP entrainée sur les données\n",
    "        Eboulis des valeurs propres\n",
    "        Heatmap\n",
    "        Retourne un tuple :\n",
    "            x[0] c'est l'objet pca\n",
    "            x[1] tableau pcs (composantes par variables)\n",
    "            x[2] c'est la projection des individus (composantes par individus)\n",
    "            x[3] c'est le df complet avec la projection\n",
    "    \"\"\"\n",
    "    \n",
    "    # Faire un module de test des paramètres d'entrée\n",
    "    # Pas de NaN, de bool ou de str dans quaque colonne du df d'entrée\n",
    "    # Scaler les données (faire une option qui le fait ou non)\n",
    "    \n",
    "    \n",
    "    # Nombre de composantes voulues, par defaut = 2\n",
    "    n_components = n_comp\n",
    "    \n",
    "    # On instancie notre PCA avec le nombre de composantes voulues\n",
    "    pca = PCA(n_components = n_comp)\n",
    "    # On l'entraine sur notre df donné en entrée\n",
    "    pca.fit(data)\n",
    "    \n",
    "    # On récupère la variance expliquée\n",
    "    scree = (pca.explained_variance_ratio_*100).round(2)\n",
    "    scree_cum = scree.cumsum().round()\n",
    "    \n",
    "    # On fait une liste des composantes calculées\n",
    "    x_list = range(1, n_comp + 1)\n",
    "    list(x_list)\n",
    "    \n",
    "    # Graphique d'éboulis des valeurs propres\n",
    "    plt.bar(x_list, scree, color = 'Grey')\n",
    "    plt.plot(x_list, scree_cum, c = 'red', marker = 'o')\n",
    "    \n",
    "    plt.xlabel(\"Rang de l'axe d'inertie\")\n",
    "    plt.ylabel(\"Pourcentage d'inertie\")\n",
    "    plt.title(\"Eboulis des valeurs propres\", fontdict = {'fontsize' : '14', 'color' : 'black', 'fontweight' : 'bold'})\n",
    "    \n",
    "    plt.show(block=False)\n",
    "    \n",
    "    # Nombre de composantes qui expliquent la variance\n",
    "    print(len(scree_cum), 'composantes expliquent', scree_cum[-1], '% de la variance')\n",
    "    \n",
    "    # Visu des composantes\n",
    "    pcs = pca.components_\n",
    "    # On met les composantes sous forme de df\n",
    "    pcs = pd.DataFrame(pcs)\n",
    "    features = data.columns\n",
    "    # On regarde les composantes par variables\n",
    "    pcs.columns = features\n",
    "    pcs.index = [f\"F{i}\" for i in x_list]\n",
    "        \n",
    "    # Heatmap sur les composantes de l'ACP\n",
    "    print('+---------------------------------------------------------------------------------------------+')\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(pcs.T, vmin=-1, vmax=1, annot=True, cmap=\"RdGy\", fmt=\"0.2f\")\n",
    "    plt.title('Heatmap Composantes / Features', fontdict = {'fontsize' : '14', 'color' : 'black', 'fontweight' : 'bold'})\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # Calcul des coordonnées de projection par individus\n",
    "    # On entraine et on transforme sur df_scaled que l'on met dans X_proj\n",
    "    X_proj = pca.fit_transform(data)\n",
    "    X_proj[:5]\n",
    "    \n",
    "    # On fait un df des composantes de nos individus sur les différents axes en vue de les rajouter dans notre CSV final\n",
    "    df_projection = pd.DataFrame(X_proj)\n",
    "    df_projection.columns = [f'F{i}' for i in x_list]\n",
    "    df_projection = df_projection.set_index(data.index)\n",
    "    df_projection\n",
    "    \n",
    "    # Faire un code qui retourne un df avec : df de base + projection des variables à la suite\n",
    "    df = pd.DataFrame(data)\n",
    "    df = pd.merge(left = df, right = df_projection, left_index = True, right_index = True, how = 'inner')\n",
    "    \n",
    "    # Retourne un tuple : x[0] c'est la pca, x[1] c'est les composantes par variables, x[2] c'est les composantes par individus, x[3] c'est le df complet avec la projection\n",
    "    return pca, pcs.T, df_projection, df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b65736c-28dc-45c3-9742-47df8d3e239f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaa30ae-0826-4805-a987-10759ab27926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09ccb66-bce4-4ef3-a072-86ff970ec4d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571e2202-26fc-491c-9499-7a4ed896f710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4b58cd-337c-4238-be42-3b40e820a2a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a86a98f-469c-4efc-8b1d-7eee9482149e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148114c2-30e4-4a5b-98c9-63571e9a1556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cb71252-4a21-4ae2-a7b6-44526fdbe82b",
   "metadata": {},
   "source": [
    "#### *ACP - Cercle de corrélation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ea96a35-ddfb-44ee-be37-918ba690d389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour cercle de corrélation\n",
    "# Il faut avoir fait l'ACP au préalable puis la passer en argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b3bb8fa-963a-49c5-af2b-c00df3d4820c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Source : TP OCR - Adaptation : DsFx\n",
    "def fct_correlation_graph(pca, x_y, features): \n",
    "    \"\"\"\n",
    "    Affiche le graphe des correlations\n",
    "\n",
    "    Positional arguments : \n",
    "    -----------------------------------------------------------------------------------------------\n",
    "    pca :\n",
    "        sklearn.decomposition.PCA : notre objet PCA qui a été fit sur nos data (quantitatives et scalées de préférence)\n",
    "        ou\n",
    "        reprendre ce qui est retourné par fct_pca sous forme xx[0] = fct_pca(data, n_comp = 2)\n",
    "    x_y :\n",
    "        list ou tuple : le couple x,y des plans à afficher, exemple [0,1] pour F1, F2\n",
    "    features :\n",
    "        list ou tuple : la liste des features (ie des dimensions) à représenter\n",
    "        ou\n",
    "        reprendre les colonnes du df passé dans l'arg pca ou xx[1].index\n",
    "    \n",
    "    return :\n",
    "    -----------------------------------------------------------------------------------------------\n",
    "        graphique du cercle des corrélations\n",
    "    \"\"\"\n",
    "\n",
    "    # Extrait x et y \n",
    "    x,y=x_y\n",
    "\n",
    "    # Taille de l'image (en inches)\n",
    "    fig, ax = plt.subplots(figsize=(10, 9))\n",
    "\n",
    "    # Pour chaque composante : \n",
    "    for i in range(0, pca.components_.shape[1]):\n",
    "\n",
    "        # Les flèches\n",
    "        ax.arrow(0,0, \n",
    "                pca.components_[x, i],  \n",
    "                pca.components_[y, i],  \n",
    "                head_width=0.07,\n",
    "                head_length=0.07, \n",
    "                width=0.02, )\n",
    "\n",
    "        # Les labels\n",
    "        plt.text(pca.components_[x, i] + 0.05,\n",
    "                pca.components_[y, i] + 0.05,\n",
    "                features[i])\n",
    "        \n",
    "    # Affichage des lignes horizontales et verticales\n",
    "    plt.plot([-1, 1], [0, 0], color='grey', ls='--')\n",
    "    plt.plot([0, 0], [-1, 1], color='grey', ls='--')\n",
    "\n",
    "    # Nom des axes, avec le pourcentage d'inertie expliqué\n",
    "    plt.xlabel('F{} ({}%)'.format(x+1, round(100*pca.explained_variance_ratio_[x],1)))\n",
    "    plt.ylabel('F{} ({}%)'.format(y+1, round(100*pca.explained_variance_ratio_[y],1)))\n",
    "\n",
    "    # J'ai copié collé le code sans le lire, et c'est tout à fait mon genre bien sur ;)\n",
    "    plt.title(\"Cercle des corrélations (F{} et F{})\".format(x+1, y+1), fontdict = {'fontsize' : '14', 'color' : 'black', 'fontweight' : 'bold'})\n",
    "\n",
    "    # Le cercle \n",
    "    an = np.linspace(0, 2 * np.pi, 100)\n",
    "    plt.plot(np.cos(an), np.sin(an))  # Add a unit circle for scale\n",
    "\n",
    "    # Axes et display\n",
    "    plt.axis('equal')\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a2a5b8-decd-4db5-afa0-a572ea38c99a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf18f4e-e2a4-4686-9a84-26d12816835c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fac445-dc7c-4427-adb3-b8f5b31cdd67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c51492b7-03d1-4638-84d0-687edd27abbf",
   "metadata": {},
   "source": [
    "#### *ACP - Projection de l'ACP*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b72fe6f-afd5-4ba7-b44c-550fe183c716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour projection des individus sur les axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de82f5df-05b8-428a-9a22-4a4b786ab2c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f17a808-5279-445c-8692-21ecde137eb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Source : TP OCR - Adaptation : DsFx\n",
    "def fct_display_factorial_planes(X_projected, x_y, pca=None, labels = None, clusters=None, alpha=1, figsize=[10,8], marker=\".\", palette = 'OrRd'):\n",
    "    \"\"\"\n",
    "    Affiche la projection des individus\n",
    "\n",
    "    Positional arguments : \n",
    "    -----------------------------------------------------------------------------------------------\n",
    "    X_projected :\n",
    "        np.array, pd.DataFrame, list of list : la matrice des points projetés,\n",
    "        recup du tuple de fct_pca[2]\n",
    "    x_y :\n",
    "        list ou tuple : le couple x,y des plans à afficher, exemple [0,1] pour F1, F2\n",
    "\n",
    "    Optional arguments : \n",
    "    -----------------------------------------------------------------------------------------------\n",
    "    pca : sklearn.decomposition.PCA : un objet PCA qui a été fit, cela nous permettra d'afficher\n",
    "    la variance de chaque composante, default = None\n",
    "    \n",
    "    labels : list ou tuple : les labels des individus à projeter, default = None\n",
    "    \n",
    "    clusters : list ou tuple : liste des clusters auquel appartient chaque individu, default = None\n",
    "    \n",
    "    alpha : float in [0,1] : transparence, 0=100% transparent, 1=0% transparent, default = 1\n",
    "    \n",
    "    figsize : list ou tuple : couple width, height qui définit la taille de la figure en inches,\n",
    "    default = [10,8] \n",
    "    \n",
    "    marker : str : le type de marker utilisé pour représenter les individus, points croix etc etc,\n",
    "    default = \".\"\n",
    "    \n",
    "    palette (str) : Le type de palette utilisé (défaut = 'OrRd')\n",
    "    \n",
    "    return :\n",
    "    -----------------------------------------------------------------------------------------------\n",
    "        graphique de la projection des individus\n",
    "    \"\"\"\n",
    "\n",
    "    # Transforme X_projected en np.array\n",
    "    X_ = np.array(X_projected)\n",
    "\n",
    "    # On définit la forme de la figure si elle n'a pas été donnée\n",
    "    if not figsize: \n",
    "        figsize = (7,6)\n",
    "\n",
    "    # On gère les labels\n",
    "    if  labels is None : \n",
    "        labels = []\n",
    "    try : \n",
    "        len(labels)\n",
    "    except Exception as e : \n",
    "        raise e\n",
    "\n",
    "    # On vérifie la variable axis \n",
    "    if not len(x_y) ==2 : \n",
    "        raise AttributeError(\"2 axes sont demandées\")   \n",
    "    if max(x_y )>= X_.shape[1] : \n",
    "        raise AttributeError(\"la variable axis n'est pas bonne\")   \n",
    "\n",
    "    # on définit x et y \n",
    "    x, y = x_y\n",
    "\n",
    "    # Initialisation de la figure       \n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "    # On vérifie s'il y a des clusters ou non\n",
    "    c = None if clusters is None else clusters\n",
    " \n",
    "    # Les points    \n",
    "    # plt.scatter(   X_[:, x], X_[:, y], alpha=alpha, \n",
    "    #                     c=c, cmap=\"Set1\", marker=marker)\n",
    "    sns.scatterplot(data=None, x=X_[:, x], y=X_[:, y], hue=c, palette = palette)\n",
    "\n",
    "    # Si la variable pca a été fournie, on peut calculer le % de variance de chaque axe \n",
    "    if pca : \n",
    "        v1 = str(round(100*pca.explained_variance_ratio_[x]))  + \" %\"\n",
    "        v2 = str(round(100*pca.explained_variance_ratio_[y]))  + \" %\"\n",
    "    else : \n",
    "        v1=v2= ''\n",
    "\n",
    "    # Nom des axes, avec le pourcentage d'inertie expliqué\n",
    "    ax.set_xlabel(f'F{x+1} {v1}')\n",
    "    ax.set_ylabel(f'F{y+1} {v2}')\n",
    "\n",
    "    # Valeur x max et y max\n",
    "    x_max = np.abs(X_[:, x]).max() *1.1\n",
    "    y_max = np.abs(X_[:, y]).max() *1.1\n",
    "\n",
    "    # On borne x et y \n",
    "    ax.set_xlim(left=-x_max, right=x_max)\n",
    "    ax.set_ylim(bottom= -y_max, top=y_max)\n",
    "\n",
    "    # Affichage des lignes horizontales et verticales\n",
    "    plt.plot([-x_max, x_max], [0, 0], color='grey', alpha=0.8)\n",
    "    plt.plot([0,0], [-y_max, y_max], color='grey', alpha=0.8)\n",
    "\n",
    "    # Affichage des labels des points\n",
    "    if len(labels) : \n",
    "        # j'ai copié collé la fonction sans la lire... Hahaha, oui, c'est tout a fait mon genre mdr (merci le petit easter egg OCR ;) )\n",
    "        # Cette fonction me reservira plus tard surement, elle fini dans ma cheat sheet ^^\n",
    "        for i,(_x,_y) in enumerate(X_[:,[x,y]]):\n",
    "            plt.text(_x, _y+0.05, labels[i], fontsize='10', ha='center', va='center') \n",
    "\n",
    "    # Titre et display\n",
    "    plt.title(f\"Projection des individus (sur F{x+1} et F{y+1})\", fontdict = {'fontsize' : '14', 'color' : 'black', 'fontweight' : 'bold'})\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07571c73-7dc7-4d31-9291-fef808f0606c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc2a040-a5a5-43e9-923b-3f27d84e037a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef4a416b-c03c-4417-a91c-450313fce8d1",
   "metadata": {},
   "source": [
    "#### *Clustering CAH -A faire*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b19596e-ba5d-4b01-bbf2-0c7f4969df07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dendrogramme\n",
    "# Diagramme d'inertie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78818bc1-0e2c-4af0-b8eb-b8a5986512a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "8fe74603-59c1-4894-a110-21ec0afdbe00",
   "metadata": {},
   "source": [
    "# On applique sur le df une CAH\n",
    "Z = linkage(df_acp, method = \"ward\", optimal_ordering = True)\n",
    "pd.DataFrame(Z)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9900997b-2acb-4275-9693-696d99a790b4",
   "metadata": {},
   "source": [
    "# Recherche du nombre optimal de clusters\n",
    "last = Z[-10:, 2]\n",
    "last_rev = last[::-1]\n",
    "idxs = np.arange(2, len(last) + 2)\n",
    "\n",
    "# Graphe en escalier de la chute d'inertie par nombre de cluster\n",
    "plt.step(idxs, last_rev, c=\"black\")\n",
    "plt.xlabel(\"Nombre de classes\")\n",
    "plt.ylabel(\"Inertie\")\n",
    "\n",
    "# Ici, on teste le scénario où il y aurait x catégories\n",
    "nombre = 5\n",
    "\n",
    "# On trace la ligne correspondant au nombre de cluster\n",
    "plt.scatter(idxs[np.where(idxs == nombre)], last_rev[np.where(idxs == nombre)], c = \"red\")\n",
    "plt.axvline(idxs[np.where(idxs == nombre)], c = \"red\", linestyle = '--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f1f1ef8-da02-4ed2-bd3f-addff92b3028",
   "metadata": {
    "tags": []
   },
   "source": [
    "# On trace le dendrogramme\n",
    "fig, ax = plt.subplots(1, 1, figsize = (16, 12))\n",
    "\n",
    "dendrogram(Z, ax = ax, labels = df_acp.index, orientation = 'left', distance_sort = 'ascending')\n",
    "\n",
    "# Tracé d'une ligne qui coupe les plus grands sauts entre clusters\n",
    "plt.axvline(x = 9, color = 'red', linestyle = 'dashed')\n",
    "\n",
    "plt.title(\"Hierarchical Clustering Dendrogram\", fontdict = {'fontsize' : '14', 'color' : 'black', 'fontweight' : 'bold'})\n",
    "plt.ylabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "plt.xlabel(\"Distance.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8bea6c-ab11-4ff6-a447-7e5870cbabec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c04df9a-3697-486c-b2b4-22cc66e4a03e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6481edb-d03b-4021-9c71-ff2255db28cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e09dc39-4505-4554-9b15-c8d2e1fb5fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5092fb-de42-4fcc-a6c0-db53747ad44f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ba8f55a-2352-4b86-9cbf-9f6fb798c3fa",
   "metadata": {},
   "source": [
    "#### *Clustering KMeans - A faire*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e551cf8-c87b-41af-933f-09c150096017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Méthode du coude\n",
    "# Score de silhouette"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c8710f13-940a-4a42-8e2a-8e4a4f38e9fe",
   "metadata": {},
   "source": [
    "# Recherche pour trouver le nombre de cluster opti\n",
    "# On part sur x cluster à tester\n",
    "k_list = range(1, 10)\n",
    "list(k_list)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7e385f67-3f6a-4a9d-8f3c-d9c15dfa0e30",
   "metadata": {},
   "source": [
    "# On défini une liste vide poury stocker l'inertie des clusters\n",
    "intertia = []\n",
    "# On entraine kmeans en boucle pour faire des cluster de 1 à x comme défini plus haut et récupérer l'inertie de chaque cluster\n",
    "for i in k_list :\n",
    "  kmeans = KMeans(n_clusters=i) # On entraine kmeans sur un nombre de cluster test\n",
    "  kmeans.fit(df_final_clean) \n",
    "  intertia.append(kmeans.inertia_) # On ajoute l'inertie de chaque cluster dans une liste"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8e74a0e9-e88f-411a-b7c1-19729b27d46d",
   "metadata": {},
   "source": [
    "# On trace l'inertie par cluster (méthode du coude)\n",
    "fig, ax = plt.subplots(1,1,figsize=(12,6))\n",
    "\n",
    "ax.set_ylabel(\"intertia\")\n",
    "ax.set_xlabel(\"n_cluster\")\n",
    "ax = plt.plot(k_list, intertia)\n",
    "\n",
    "# Ici, on teste le scénario où il y aurait x catégories\n",
    "nombre = 5\n",
    "plt.axvline(nombre, c = \"red\", linestyle = '--')\n",
    "\n",
    "plt.title('Inertie par nombre de cluster', fontdict = {'fontsize' : '14', 'color' : 'black', 'fontweight' : 'bold'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "518eec2d-e788-4050-8f1b-6a35813e7f99",
   "metadata": {},
   "source": [
    "# Calcul du score de silhouette\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "\n",
    "for i in k_list :\n",
    "    # Instanciation de kmeans\n",
    "    km = KMeans(n_clusters=i+1)\n",
    "\n",
    "    # Entrainement sur le model\n",
    "    km.fit_predict(df_acp)\n",
    "\n",
    "    # Calcul du score de silhouette\n",
    "    score = silhouette_score(df_acp, km.labels_, metric='euclidean')\n",
    "\n",
    "    # Résultat\n",
    "    print('Nombre cluster :', i+1, '--> Silhouette Score = %.3f' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0225873-9f29-4b8f-85f0-926308abc86e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eec110-9de2-40b8-9768-554770259af0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0726b3ca-750d-471a-a1db-ae13c2de7229",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550f5e4c-d7dc-42ab-a0f4-33ba445f2641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03a1613f-1fed-4892-aea6-398f3d9914d9",
   "metadata": {},
   "source": [
    "#### *Grid Search*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b103244-4d84-48ec-8638-78a529077089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605996db-70b8-4cb6-ab18-aee4376a024b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db22a07-bbbc-461a-beb0-45482ac245a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7118b813-12a5-4dd6-a8c5-2f4025520b1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b774577e-fc9f-4158-b217-601e8b592c11",
   "metadata": {},
   "source": [
    "#### *Regression Linéaire*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb83373-da35-4fa5-9646-f703a69eed42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327dc555-90d1-4a70-9800-e2a7b9907f12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bfc4df7-5f91-4193-9daf-91b1ff428491",
   "metadata": {},
   "source": [
    "#### *Regression logistique*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e112d1d2-afad-41d4-805c-3cede501a604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746346de-8a18-4af5-ab07-89988465ce4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e49db198-aae2-4b90-b4b1-00a83f8b4d72",
   "metadata": {},
   "source": [
    "#### *KNN*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc47375-dccd-452c-a482-92377fa1c540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fba24f-20f4-4eac-b72f-5fd3a7cf82d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f339af5f-ecd5-476d-946f-aadb49959fec",
   "metadata": {},
   "source": [
    "#### *Recherche de variables explicatives pour une régression linéaire multi*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e884d848-18a1-477c-bc7a-4c553118ac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source : TP OCR - Adaptation : DsFx\n",
    "def fct_back_select_linear(data, response, summary = False):\n",
    "    \"\"\"\n",
    "    Linear model designed by backward selection\n",
    "\n",
    "    Positional arguments : \n",
    "    -----------------------------------------------------------------------------------------------\n",
    "    data (pd.DataFrame) :\n",
    "        DataFrame avec toute les variables et la réponse  (valeur à prédire)\n",
    "\n",
    "    response (str) :\n",
    "        Nom de la colonne de réponse dans data, ce qui est conservé\n",
    "    \n",
    "    summary (bool) :\n",
    "        Affiche les différentes étapes du process de sélection\n",
    "\n",
    "    return :\n",
    "    -----------------------------------------------------------------------------------------------\n",
    "    model: modèle \"optimal\" entrainé avec statsmodel\n",
    "           sélection par élimination des paramètres non-significatifs (backward selection)\n",
    "           évaluation par p-value\n",
    "           \n",
    "    Infos :\n",
    "    -----------------------------------------------------------------------------------------------\n",
    "    Fonction qui va analyser les variables explicatives en fonction de la variable recherchée\n",
    "    Au fur et à mesure va supprimer des variables afin de ne conserver que les paramètres\n",
    "    significatifs\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Recherche de variables explicatives pertinentes pour :', response)\n",
    "    print('')\n",
    "    \n",
    "    # Retire la colonne 'response' et défini le paramètre de sortie de la boucle\n",
    "    remaining = set(data._get_numeric_data().columns)\n",
    "    if response in remaining:\n",
    "        remaining.remove(response)\n",
    "    cond = True\n",
    "    \n",
    "    # Boucle qui va tester les variables au fur et à mesure\n",
    "    while remaining and cond:\n",
    "        # On prend les différentes variables\n",
    "        formula = \"{} ~ {} + 1\".format(response,' + '.join(remaining))\n",
    "        # Déroulement de la sélection par suppression des paramètres non-significatifs\n",
    "        print('_______________________________')\n",
    "        print(formula)\n",
    "        # Train de ols\n",
    "        model = smf.ols(formula, data).fit()\n",
    "        # Récupération des p-values\n",
    "        score = model.pvalues[1:]\n",
    "        \n",
    "        # Recherche de la p-values la plus élevée\n",
    "        # Si p-values > à 0.05, le paramètre est retiré\n",
    "        # Si toutes les p-values sont < 0.05, on arrête la boucle\n",
    "        \n",
    "        if summary == True:\n",
    "            print(model.summary()) # Pour suivi de l'évolution de recherche, peut être passé en commentaire si bcp de paramètres à tester\n",
    "        \n",
    "        toRemove = score[score == score.max()]\n",
    "        if toRemove.values > 0.05:\n",
    "            # Résumé de la recherche\n",
    "            print('remove', toRemove.index[0], '(p-value :', round(toRemove.values[0],3), ')')\n",
    "            remaining.remove(toRemove.index[0])\n",
    "        else:\n",
    "            # Donne le modèle final le plus pertinent\n",
    "            cond = False\n",
    "            print('')\n",
    "            print('+============================================================================+')\n",
    "            print('|                        Le modèle final !  (summary)                        |')\n",
    "            print('+============================================================================+')\n",
    "        print('')\n",
    "    # Information sur le modèle\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2c9168-bf97-4a61-888b-cc778e4be5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7870dce-78db-4a9d-b74c-cef1bc91ba41",
   "metadata": {},
   "source": [
    "#### *Recherche de variables explicatives pour une régression logistique*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59fea6a8-a926-4135-a3cd-3c341bb3df0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source : TP OCR - Adaptation : DsFx (la fonction smf. change uniquement)\n",
    "def fct_back_select_logit(data, response, summary = False):\n",
    "    \"\"\"\n",
    "    Logistic model designed by backward selection\n",
    "\n",
    "    Positional arguments : \n",
    "    -----------------------------------------------------------------------------------------------\n",
    "    data (pd.DataFrame) :\n",
    "        DataFrame avec toute les variables et la réponse (valeur à prédire)\n",
    "\n",
    "    response (str) :\n",
    "        Nom de la colonne de réponse dans data, ce qui est conservé\n",
    "    \n",
    "    summary (bool) :\n",
    "        Affiche les différentes étapes du process de sélection\n",
    "\n",
    "    return :\n",
    "    -----------------------------------------------------------------------------------------------\n",
    "    model: modèle \"optimal\" entrainé avec statsmodel\n",
    "           sélection par élimination des paramètres non-significatifs (backward selection)\n",
    "           évaluation par p-value\n",
    "           \n",
    "    Infos :\n",
    "    -----------------------------------------------------------------------------------------------\n",
    "    Fonction qui va analyser les variables explicatives en fonction de la variable recherchée\n",
    "    Au fur et à mesure va supprimer des variables afin de ne conserver que les paramètres\n",
    "    significatifs\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Recherche de variables explicatives pertinentes pour :', response)\n",
    "    print('')\n",
    "    \n",
    "    # Retire la colonne 'response' et défini le paramètre de sortie de la boucle\n",
    "    remaining = set(data._get_numeric_data().columns)\n",
    "    if response in remaining:\n",
    "        remaining.remove(response)\n",
    "    cond = True\n",
    "    \n",
    "    # Boucle qui va tester les variables au fur et à mesure\n",
    "    while remaining and cond:\n",
    "        # On prend les différentes variables\n",
    "        formula = \"{} ~ {} + 1\".format(response,' + '.join(remaining))\n",
    "        # Déroulement de la sélection par suppression des paramètres non-significatifs\n",
    "        print('_______________________________')\n",
    "        print(formula)\n",
    "        # Train de ols\n",
    "        model = smf.logit(formula, data).fit()\n",
    "        # Récupération des p-values\n",
    "        score = model.pvalues[1:]\n",
    "        \n",
    "        # Recherche de la p-values la plus élevée\n",
    "        # Si p-values > à 0.05, le paramètre est retiré\n",
    "        # Si toutes les p-values sont < 0.05, on arrête la boucle\n",
    "        \n",
    "        if summary == True:\n",
    "            print(model.summary()) # Pour suivi de l'évolution de recherche, peut être passé en commentaire si bcp de paramètres à tester\n",
    "        \n",
    "        toRemove = score[score == score.max()]\n",
    "        if toRemove.values > 0.05:\n",
    "            # Résumé de la recherche\n",
    "            print('remove', toRemove.index[0], '(p-value :', round(toRemove.values[0],3), ')')\n",
    "            remaining.remove(toRemove.index[0])\n",
    "        else:\n",
    "            # Donne le modèle final le plus pertinent\n",
    "            cond = False\n",
    "            print('')\n",
    "            print('+============================================================================+')\n",
    "            print('|                        Le modèle final !  (summary)                        |')\n",
    "            print('+============================================================================+')\n",
    "        print('')\n",
    "    # Information sur le modèle\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59dbfdb-94a8-4b7b-b2cd-9a174a7d6c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353d7174-d35e-4bfb-8998-64f0db2fabfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4fa671-f5c7-4796-b00a-43d0dc7f7170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b22d3f-28d9-49f2-a105-0a88d666f5b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24877c5c-cca8-433b-bda0-bfdef5a755cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2554dc0b-b91f-4367-a678-ec203e5a6b96",
   "metadata": {},
   "source": [
    "#### *Vérification lancement des fonctions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e339d6a-63af-49b0-86c3-93b9b70400a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cette fonction doit rester à la fin de ce notebook et être exécutée après le chargement de celui dans un notebook afin d'être sur qu'il soit importer en mémoire\n",
    "def fct_load():\n",
    "    print('')\n",
    "    print('+--o=      Test run DsFx_fct      =o--+')\n",
    "    print('')\n",
    "    print('+-------------------------------------+')\n",
    "    print('|   Import lib : OK - Loaded          |')\n",
    "    print('|      Run fct : OK - Loaded          |')\n",
    "    print('+-------------------------------------+')\n",
    "    print('                                       ')\n",
    "    print('_______________________________________')\n",
    "    print(\" _    __    _                          \")\n",
    "    print(\"| \\ _|_   _|___|_  o  | _  _  _| _  _| \")\n",
    "    print(\"|_/_>|><___|(_ |_  o  |(_)(_|(_|(/_(_| \")\n",
    "    print(\" _                                 ... \")\n",
    "    print(\"|_) _  _  _|\\/ _|_ _      _ _      ||| \")\n",
    "    print(\"| \\(/_(_|(_|/   |_(_) |_|_>(/_     000 \")\n",
    "    print('_______________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7663f9a4-496d-4fdd-a5d2-07ff78641575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coding by : David GESSER\n",
    "# 28/12/2023"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
